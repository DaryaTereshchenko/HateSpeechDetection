{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/0FDRWX7fFijCvCZzBfjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaryaTereshchenko/HateSpeechDetection/blob/main/Emb_Roberta_Sm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTqN8HTxwzoj"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "T3KPkhPow3JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, TextVectorization, Input"
      ],
      "metadata": {
        "id": "6oapDCthw7SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ethos_hate = pd.read_csv(\"ethos_hate.csv\", sep=\",\", encoding=\"unicode_escape\").drop(columns=\"label\")\n",
        "ethos_hate[\"label\"] = 1\n",
        "\n",
        "ethos_neutral = pd.read_csv(\"ethos_neutral.csv\", sep=\",\", encoding=\"unicode_escape\").drop(columns=\"label\")\n",
        "ethos_neutral[\"label\"] = 0\n",
        "\n",
        "train_n = ethos_neutral.sample(frac=0.9, random_state=0)\n",
        "test_n = ethos_neutral.drop(train_n.index)\n",
        "\n",
        "train_hate = ethos_hate.sample(frac=0.9, random_state=0)\n",
        "test_hate = ethos_hate.drop(train_hate.index)\n",
        "\n",
        "train = pd.concat([train_n, train_hate]).sample(frac=1, random_state=300).reset_index(drop=True)\n",
        "test = pd.concat([test_n, test_hate]).sample(frac=1, random_state=300).reset_index(drop=True)\n",
        "\n",
        "print(train.text[0])\n"
      ],
      "metadata": {
        "id": "YuwP2uwjxBuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hate_speech = pd.read_csv('ethos_sm_hate.csv', sep=\";\", decimal=\",\", skiprows=[1,2,3,4], low_memory=False, on_bad_lines='skip').sort_values(by=[\"CODE\"])\n",
        "neutral_speech = pd.read_csv('ethos_sm_neutral.csv', sep=\";\", decimal=\",\", skiprows=[1,2,3,4], low_memory=False, on_bad_lines='skip').sort_values(by=[\"CODE\"])\n",
        "\n",
        "hate_speech = hate_speech.drop(columns=\"CODE\")\n",
        "neutral_speech = neutral_speech.drop(columns=\"CODE\")"
      ],
      "metadata": {
        "id": "-I-7cvtRxCVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sm_n = neutral_speech.sample(frac=0.9, random_state=0)\n",
        "test_sm_n = neutral_speech.drop(train_sm_n.index)\n",
        "\n",
        "train_sm_hate = hate_speech.sample(frac=0.9, random_state=0)\n",
        "test_sm_hate = hate_speech.drop(train_sm_hate.index)\n",
        "\n",
        "TrainSM = pd.concat([train_sm_n, train_sm_hate]).sample(frac=1, random_state=300).reset_index(drop=True)\n",
        "TestSM = pd.concat([test_sm_n, test_sm_hate]).sample(frac=1, random_state=300).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "nnTg9PTWxF3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(line):\n",
        "  processed_line = line.lower()\n",
        "  words = [word for word in word_tokenize(processed_line)]\n",
        "  processed_line = [word for word in words if word not in set(string.punctuation)]\n",
        "  text = \" \".join(processed_line)a\n",
        "  return text"
      ],
      "metadata": {
        "id": "0jR8Vzb9xLY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences for embeddings\n",
        "train_sentence_list = np.asarray(list((map(clean_text, train.text.values))))\n",
        "test_sentence_list = np.asarray(list(map(clean_text, test.text.values)))"
      ],
      "metadata": {
        "id": "OuWdhbthxWte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hurtlex_EN.tsv', sep='\\t')\n",
        "vocab = df[\"lemma\"].unique()"
      ],
      "metadata": {
        "id": "MPL1MqZzxamz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "max_features = 6000  # maximum word number\n",
        "embedding_dims = 20"
      ],
      "metadata": {
        "id": "ORe28IA8xeWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "8uZzI23JxsmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roberta_encode(texts, tokenizer):\n",
        "    ct = len(texts)\n",
        "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
        "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
        "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
        "\n",
        "    for k, text in enumerate(texts):\n",
        "        # Tokenize\n",
        "        tok_text = tokenizer.tokenize(text)\n",
        "        \n",
        "        # Truncate and convert tokens to numerical IDs\n",
        "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
        "        \n",
        "        input_length = len(enc_text) + 2\n",
        "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
        "        \n",
        "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
        "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
        "        \n",
        "        # Set to 1s in the attention input\n",
        "        attention_mask[k,:input_length] = 1\n",
        "\n",
        "    return {\n",
        "        'input_word_ids': input_ids,\n",
        "        'input_mask': attention_mask,\n",
        "        'input_type_ids': token_type_ids\n",
        "    }"
      ],
      "metadata": {
        "id": "QQ2Tc3v8xwJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict = roberta_encode(train_sentence_list, tokenizer)\n",
        "train_dict[\"np_sm\"] = np.asarray(TrainSM.values, dtype=\"float32\")\n",
        "\n",
        "test_dict = roberta_encode(test_sentence_list, tokenizer)\n",
        "test_dict[\"np_sm\"] = np.asarray(TestSM.values,  dtype=\"float32\")"
      ],
      "metadata": {
        "id": "zsJz5nBr0jsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_dict\n",
        "X_test = test_dict\n",
        "\n",
        "y_train = np.asarray(train.label, dtype='int32').reshape(-1,1)\n",
        "y_test = np.asarray(test.label, dtype='int32').reshape(-1,1)"
      ],
      "metadata": {
        "id": "XzhvgN4Sx0nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "  input_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
        "  input_type_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
        "  np_sm = Input(shape=(157,), dtype=tf.float32, name='np_sm')\n",
        "  sents = Input(shape=(), dtype=tf.string)\n",
        "  \n",
        "\n",
        "  roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)\n",
        "  x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
        "  x = x[0]\n",
        "  np_sm = np_sm\n",
        "\n",
        "  vectorize_layer = TextVectorization(max_tokens=max_features, output_mode='int', output_sequence_length=MAX_LEN, vocabulary=vocab)(sents)\n",
        "  y = Embedding(input_dim=max_features, output_dim=128, input_length=MAX_LEN)(vectorize_layer)\n",
        "\n",
        "  x = tf.keras.layers.Dropout(rate=0.3)(x)\n",
        "  x = tf.keras.layers.Falatten()(x)\n",
        "  x = tf.keras.layers.Dense(128)(x)\n",
        "\n",
        "  y = tf.keras.layers.Dropout(rate=0.5)(y)\n",
        "  y = tf.keras.layers.Falatten()(y)\n",
        "  y = tf.keras.layers.Dense(128)(y)\n",
        "\n",
        "  z = tf.keras.layers.concatenate([x, np_sm, y])\n",
        "  z = tf.keras.layers.Dense(32, activation='relu')(z)\n",
        "  z =  tf.keras.layers.BatchNormalization()(z)\n",
        "  z = tf.keras.layers.Dense(1, activation='sigmoid')(z)\n",
        "  \n",
        "  model = Model(inputs=[input_word_ids, input_mask, input_type_ids, np_sm, sents], outputs=z)\n",
        "\n",
        "  # x1.trainable = True\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "      loss= tf.keras.losses.binary_crossentropy,\n",
        "      metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "u7whrig8xiqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model1()\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "PgeKIMZw0uYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training...')\n",
        "history = model.fit([X_train, train_sentence_list],\n",
        "                    y_train,\n",
        "                    epochs=25,\n",
        "                    batch_size=16,\n",
        "                    verbose=1,\n",
        "                    validation_data=([X_test, test_sentence_list], y_test))"
      ],
      "metadata": {
        "id": "3wZ8DHO501mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This plot will look much better if we train models with more epochs, but anyway here is\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title('Accuracy')\n",
        "\n",
        "xaxis = np.arange(len(history.history['accuracy']))\n",
        "plt.plot(xaxis, history.history['accuracy'], label='Train set')\n",
        "plt.plot(xaxis, history.history['val_accuracy'], label='Validation set')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "YWZmLP0C18wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
      ],
      "metadata": {
        "id": "Ffj7Xj0O19Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [int(np.round(i,0)) for i in model.predict(X_test)]"
      ],
      "metadata": {
        "id": "7_vsDTcy1_Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "xQgTXKYu2BBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "con_mat_df = confusion_matrix(y_test, y_pred)\n",
        "con_mat_df = con_mat_df.astype('float') / con_mat_df.sum(axis=1)[:, np.newaxis]"
      ],
      "metadata": {
        "id": "p63FZ5XZ2Crm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "figure = plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(con_mat_df, cmap=plt.cm.Blues, annot=True)\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "print(con_mat_df)"
      ],
      "metadata": {
        "id": "cC0LslgA2Cwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}